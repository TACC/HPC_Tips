##----
To restrict the memory that a JVM can use on a Stampede login node, try the following: "export JAVA_TOOL_OPTIONS="-Xms2G -Xmx2G".
##----
Don't get be surprised by scheduled maintenance - subscribe to announcements at https://www.tacc.utexas.edu/news/subscribe.
##----
Your MPI jobs suddenly start failing with mpispawn errors? Your rsa keys may be corrupted: to regenerate, go to $HOME then execute "mv .ssh dot.ssh.old", logout, then log back in.
##----
OMP_NUM_THREADS defaults to 1 on Lonestar and Stampede hosts (244 on MIC); be sure to export the desired value before running OpenMP jobs.
##----
MIC_OMP_NUM_THREADS defaults to 240 (offload) and 244 (native launched from host); be sure to export the desired value before running OpenMP jobs involving the Xeon Phi MIC.
##----
Did you know TACC offers training courses for HPC? http://www.tacc.utexas.edu/user-services/training
##----
HPC Python? Yes! We have the Enthought distribution: "module load python".
##----
A newer version of the system-supplied Perl is available by running the command "module load perl"
##----
It usually takes 30 minutes or more for new passwords to propagate across TACC systems.
##----
If you have trouble submitting jobs, make sure that your $HOME and $HOME/.ssh directories have the right permissions.  The following commands may remedy the situation:
  $ chmod go-w $HOME $HOME/.ssh
  $ chmod 600 $HOME/.ssh/authorized_keys
  $ chown `whoami` $HOME/.ssh/authorized_keys
##----
If you need X11 to run a GUI-based app on a Stampede compute node, login with "ssh -X", use idev rather than srun, and make sure you use the development queue.
##----
$HOME, $WORK, and $SCRATCH are available to native MIC codes (and to users who ssh to a MIC), but are not accessible from offloaded code. The system aliases cdw and cds are not available on the MIC.
##----
View pdf files on stampede with xpdf.
##----
Linking to TACC packages? You don't have to remember their locations. Just use $TACC_THATPACKAGE_DIR, $TACC_THATPACKAGE_INC, and $TACC_THATPACKAGE_LIB
##----
Want to keep tabs on your job?
   $ watch -n 10 squeue -u yourname
##----
Not sure if TACC has a package you're looking for?
See https://www.tacc.utexas.edu/resources/software.
##----
Not sure which TACC machine you're logged into? Type "hostname -f".
##----
We perform a full backup of your $HOME directory every few months, and an incremental backup of the latest changes every few days. To recover a file submit a ticket. Please provide the name of the file(s), the date of most useful version, and an estimate of the time you lost the file.
##----
To see MVAPICH2's process mapping, "export MV2_SHOW_CPU_BINDING=1" inside your script or before launching your job.
##----
A code built with Intel MPI won't run with MVAPICH2 and vice versa. Your build environment must match your submit environment.
##----
If you have an MPI application with threads, don't forget to use tacc_affinity. See the system user guide for more details. 
##----
The Intel compilers are only usable from the login nodes and the development queue.
##----
On a Lustre file system like $WORK, and $SCRATCH, use "lfs find" instead of "find" to search for files.  It is much faster:
    $ lfs find <directory/file>
##----
When in doubt, read the manual: http://www.tacc.utexas.edu/user-services/user-guides/stampede-user-guide
##----
The login nodes are a shared resource, so please don't use mpirun or multiple tar's or multiple file transfers or "make -j4" there.
##----
Does rank 0 need more memory than your other MPI tasks? If you have 64 tasks, for example, allocate 5 nodes and launch with:
  $ ibrun -n 64 -o 15  # puts rank 0 on a node by itself.
##----
Do you know what is cool about cdh?
"cd vtune" tries to cd to the vtune subdirectory in the present directory where as "cdh vtune"  will take you to $HOME/vtune,  similar to the behavior for an argument with cds, cdw. You cannot do that with cd (as it should be),   It is quite handy.  (note that cd ~/vtune does the same thing).
##----
In the commands "srun -n X ..." or "idev -n X" or "#SBATCH -n X", X correspond to the number of MPI tasks that you want to use, not the number of cores.
For example, on Stampede,  if you want to use 1 node with 4 MPI tasks and 4 OpenMP threads, please use "srun -n 4..." or "idev -n 4" or "#SBATCH -n 4"
##----
By default, idev provides you a full node and ibrun will use all cores as MPI tasks. For example, on Stampede, If you want to use 1 node on with 4 MPI tasks and 4 OpenMP threads, please use "idev -n 4"
##----
Do you want to use a hybrid code on 8 nodes using 2 MPI tasks and 8 OpenMP threads per node on Stampede?
Be sure  to use the "-N" option to specify the number of nodes that you want to use in addition to "-n".
In this example, it will be "srun -N 8 -n 16..." or in your script "#SBATCH -N 8 \ #SBATCH -n 16"
##----
