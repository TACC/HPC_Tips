##----
To restrict the memory that a JVM can use on a Stampede login node, try the following:
   $ export JAVA_TOOL_OPTIONS="-Xms2G -Xmx2G".
##----
Don't get be surprised by scheduled maintenance - subscribe to announcements at https://www.tacc.utexas.edu/news/subscribe.
##----
Are your MPI jobs suddenly failing with mpispawn errors? Your rsa keys may be corrupted: to regenerate, go to $HOME then execute "mv .ssh dot.ssh.old", logout, then log back in.
##----
OMP_NUM_THREADS defaults to 1 on Lonestar and Stampede hosts (244 on MIC); be sure to export the desired value before running OpenMP jobs.
##----
MIC_OMP_NUM_THREADS defaults to 240 (offload) and 244 (native launched from host); be sure to export the desired value before running OpenMP jobs involving the Xeon Phi MIC.
##----
Did you know TACC offers training courses for HPC? http://www.tacc.utexas.edu/user-services/training
##----
HPC Python? Yes! "module load python".
##----
Want to install your own python modules?  Do "module help python".
##----
A newer version of the system-supplied Perl is available by running the command "module load perl"
##----
It usually takes 30 minutes or more for new passwords to propagate across TACC systems.
##----
If you have trouble submitting jobs, make sure that your $HOME and $HOME/.ssh directories have the right permissions.  The following commands may remedy the situation:
  $ chmod go-w $HOME $HOME/.ssh
  $ chmod 600 $HOME/.ssh/authorized_keys
  $ chown `whoami` $HOME/.ssh/authorized_keys
##----
If you need X11 to run a GUI-based app on a Stampede compute node, login with "ssh -X", use idev rather than srun, and make sure you use the development queue.
##----
You can access your home, work, and scratch directories from native MIC codes, but you cannot do so from offloaded code. The environment variables $WORK and $SCRATCH (and the aliases cdw and cds) are not defined on the MIC.
##----
View pdf files on stampede with xpdf.
##----
Linking to TACC packages? You don't have to remember their locations. Just use $TACC_THATPACKAGE_DIR, $TACC_THATPACKAGE_INC, and $TACC_THATPACKAGE_LIB.  These variables will be defined after you load the module.
##----
Want to keep tabs on your job?
   $ watch -n 10 squeue -u yourname
##----
Not sure if TACC has a package you're looking for? See https://www.tacc.utexas.edu/resources/software.
##----
Not sure which TACC machine you're logged into? Type "hostname -f".
##----
We perform a full backup of your $HOME directory every few months, and an incremental backup of the latest changes every few days. To recover a file submit a ticket. Please provide the name of the file(s), the date of most useful version, and an estimate of the time you lost the file.
##----
To see MVAPICH2's process mapping, "export MV2_SHOW_CPU_BINDING=1" inside your script or before launching your job.
##----
A code built with Intel MPI won't run with MVAPICH2 and vice versa. Your build environment must match your submit environment.
##----
If you have an MPI application with threads, don't forget to use tacc_affinity. See the system user guide for more details. 
##----
The Intel compilers are only usable from the login nodes and the development queue.
##----
On a Lustre file system like $WORK, and $SCRATCH, use "lfs find" instead of "find" to search for files.  It is much faster:
    $ lfs find <directory> -name '*.c'
##----
When in doubt, read the manual: http://www.tacc.utexas.edu/user-services/user-guides/stampede-user-guide
##----
The login nodes are a shared resource, so please don't use mpirun or multiple tar's or multiple file transfers or "make -j4" there.
##----
Does rank 0 need more memory than your other MPI tasks? If you have 64 tasks, for example, allocate 5 nodes and launch with:
  $ ibrun -n 64 -o 15  # puts rank 0 on a node by itself.
##----
Do you know what is cool about cdh?

"cd vtune" tries to cd to the vtune subdirectory in the present directory where as "cdh vtune"  will take you to $HOME/vtune,  similar to the behavior for an argument with cds, cdw. You cannot do that with cd (as it should be),   It is quite handy.  (note that cd ~/vtune does the same thing).
##----
In the commands "srun -n X ..." or "idev -n X" or "#SBATCH -n X", X correspond to the number of MPI tasks that you want to use, not the number of cores.

For example, on Stampede,  if you want to use 1 node with 4 MPI tasks and 4 OpenMP threads, please use "srun -n 4..." or "idev -n 4" or "#SBATCH -n 4"
##----
By default, idev provides you a full node and ibrun will use all cores as MPI tasks. For example, on Stampede, If you want to use 1 node on with 4 MPI tasks and 4 OpenMP threads, please use "idev -n 4"
##----
Do you want to use a hybrid code on 8 nodes using 2 MPI tasks and 8 OpenMP threads per node on Stampede? Be sure  to use the "-N" option to specify the number of nodes that you want to use in addition to "-n".

In this example, it will be "srun -N 8 -n 16..." or in your script "#SBATCH -N 8 \\ #SBATCH -n 16"
##----
Here's a great way to set permissions recursively to share a directory named projdir with your research group:
 
    $ lfs find projdir | xargs chmod g+rX
 
Using lfs is faster and less stressful on Lustre than a recursive chmod. The capital "X" assigns group execute permissions only to files and directories for which the owner has execute permissions.
##----
You can transfer files to your work directory on another machine by putting a '\\' in front of $WORK:
   $ scp yourfile stampede:\\$WORK
##----
It is probably safer to use "module reset" rather than "module purge" to get to a known good state.  At TACC, "module reset" means "module purge; module load TACC".
##----
The most portable way across TACC systems to load a particular compiler and MPI stack combination in a script is to unload the currently loaded compiler and MPI stack first using environment variables. Then you may load the specific compiler and MPI stack that you want. E.g.:
   $ module unload $TACC_FAMILY_MPI $TACC_FAMILY_COMPILER
   $ module load <compiler>/version <mpi>/version
   $ module load <PKG1> <PKG2>
##----
At TACC, loading any intel or gcc compiler module will define TACC_FAMILY_COMPILER to be "intel" or "gcc". This can be useful in Makefiles to control which compiler flags to set.
##----
At TACC, loading any MPI module will define TACC_FAMILY_MPI to be the name of the MPI Stack name (e.g mvapich2, impi).
##----
The day before a Tuesday maintenance is a great time to submit short jobs. The scheduler drains the queues by holding up jobs that will not complete before the start of maintenance. Your short jobs will have the machine to themselves!
##----
To perform basic diagnostics on your account settings, execute "module load sanitytool" and then run "sanitycheck -v".
##----
If you don't need 48 hrs for your job, don't request that much: the job scheduler will have an easier time finding a slot for the 1 hour you really need.
##----
Would you like to know how much memory your code uses during execution? Try TACC's remora tool:
  $ module load remora
  $ module help remora
##----
Would you like to know how much IO load is generated by your code? Try TACC's remora tool:
  $ module load remora
  $ module help remora
##----
Curious about the network topology in your job? Try TACC's remora tool:
  $ module load remora
  $ module help remora
##----
Did you know that job resource utilization reports are available via TACC's remora tool? Try it:
  $ module load remora
  $ module help remora
##----
MPI jobs running out of memory? Spread your tasks across more nodes to increase the memory available to each task. Instead of “-N 4 –n 64” (64 tasks spread across 4 nodes, or 16 tasks per node), try “-N 16 –n 64” (4 tasks per node) or even “-N 64 –n 64” (1 task per node). Use the remora module to monitor your job’s memory use.
##----
